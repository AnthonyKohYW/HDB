{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "95bcdd1b-c70c-4df2-8b80-a8f17d0293b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================\n",
    "# STEP 1: IMPORTS & PATH CONFIG\n",
    "# =========================================\n",
    "\n",
    "# Standard libraries\n",
    "import os\n",
    "import requests\n",
    "from datetime import datetime\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import time\n",
    "from io import StringIO\n",
    "\n",
    "# Data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Hashing\n",
    "import hashlib\n",
    "\n",
    "# ---------------------------\n",
    "# FILE PATHS / OUTPUT FOLDERS\n",
    "# ---------------------------\n",
    "base_path = \"HDB_Data\"  # base folder for all outputs\n",
    "folders = ['Raw', 'Cleaned', 'Transformed', 'Failed', 'Hashed']\n",
    "\n",
    "# Create folders if they don't exist\n",
    "for f in folders:\n",
    "    os.makedirs(os.path.join(base_path, f), exist_ok=True)\n",
    "\n",
    "# Optional: file names for saving later\n",
    "raw_file = os.path.join(base_path, 'Raw', 'hdb_raw.csv')\n",
    "cleaned_file = os.path.join(base_path, 'Cleaned', 'hdb_cleaned.csv')\n",
    "transformed_file = os.path.join(base_path, 'Transformed', 'hdb_transformed.csv')\n",
    "failed_file = os.path.join(base_path, 'Failed', 'hdb_failed.csv')\n",
    "hashed_file = os.path.join(base_path, 'Hashed', 'hdb_hashed.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2b2d9932-bd26-464e-a8a3-4d54a52a1002",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[d_2d5ff9ea31397b66239f245f57751537] Download initiated.\n",
      "[d_2d5ff9ea31397b66239f245f57751537] Poll rate limit hit. Waiting 5s...\n",
      "[d_2d5ff9ea31397b66239f245f57751537] Poll rate limit hit. Waiting 5s...\n",
      "[d_2d5ff9ea31397b66239f245f57751537] File ready: https://s3.ap-southeast-1.amazonaws.com/table-downloads-ingest.data.gov.sg/d_2d5ff9ea31397b66239f245f57751537/89c9b958f7e52a1c0ad9763e0e0e63f06d777007346366bc293e3c395e552bd7.csv?AWSAccessKeyId=ASIAU7LWPY2WC7TKX2J6&Expires=1771227008&Signature=yBAl56NJkrobAe3GTYe6yh7EYGU%3D&X-Amzn-Trace-Id=Root%3D1-6992b96e-0dd870ac0be0ed2172547469%3BParent%3D7eec3d4f4e230299%3BSampled%3D0%3BLineage%3D1%3Affb76583%3A0&response-content-disposition=attachment%3B%20filename%3D%22ResaleFlatPricesBasedonRegistrationDateFromMar2012toDec2014.csv%22&x-amz-security-token=IQoJb3JpZ2luX2VjEGYaDmFwLXNvdXRoZWFzdC0xIkcwRQIgOtbRko2h3dJDZC7Bv97SPtVFlReLBSL127YYBlt%2B860CIQCqobuoIVa2NdmaR7%2F5xSgjiQumfjjmmP6JorQel1wV9yqqAwgvEAQaDDM0MjIzNTI2ODc4MCIMcS0WnxdkxBAydaMTKocDAGcuX%2FhUFixYUXWwwvty1fBaxWkVoniuq76W5EnzTOUVAEIb8Q55X0XRkkkHvNtPQCUU6W1pUB6uYlg7gK9WgfnbR0GIL1CBOXcv5d18GwIQrV8QCtnawj%2FvaWVsXCm%2Be4I8%2FnOrKYs4JVkKc%2BY7diS%2FsDQLmDPJS9ocrOApdGs%2BNMbPsr37TQa9uC7T6oZddJyhusceTSlLLM0bULF75cXmlmIliZQJ4KQIQIEM9cf88jQoGTOATkJkZsC%2FlsRvMqfab37eFthhEJYoee9dbiUJu9EmCGAtbSUGVqjpo6f3c2RzvllVSHOGUkDCEEel25yE%2FkRCAgaTLub3DTuG6g0p0RQEadLT7mb2bPnF7B1CtQ46bzovvYislYeG4bmn9uusQ6HZ5kxkv%2FDFLEZ%2F32qQ9nBihz43Uhr2l9XiZTrwqeOnHrukefLVlA3m4zzN654%2Fn0NexK%2Bfh%2BZzSLyUp1EFmb2hKApVFF44HDGsxujGIg%2FcDFkE7wdTT78b4IcaHzJoSqGuiDDe7srMBjqdAYl5Vm8PXteak06MdNJM%2B2kD1i%2Fwa%2Fq38J9fyd5heXGRV8ebPyekDE6QJQBbrTqXs9BBvztR8xPhU5ZLTUkvvIHXC8E4GT9RnAZNNkoAwu7zQuBtYXP5O2o9x4S1xr6QrGrqMEm%2FKIJaArvfJo9M6I942%2B7cWliqjFo8a%2FHLVzHvHOQ1MToAfrOh1%2BaWhKHbAC5qm8gDd%2FcXVB9POIM%3D\n",
      "[d_ea9ed51da2787afaf8e51f827c304208] Initiate rate limit hit. Waiting 10s... (Attempt 1/10)\n",
      "[d_ea9ed51da2787afaf8e51f827c304208] Initiate rate limit hit. Waiting 10s... (Attempt 2/10)\n",
      "[d_ea9ed51da2787afaf8e51f827c304208] Download initiated.\n",
      "[d_ea9ed51da2787afaf8e51f827c304208] Poll rate limit hit. Waiting 5s...\n",
      "[d_ea9ed51da2787afaf8e51f827c304208] File ready: https://s3.ap-southeast-1.amazonaws.com/table-downloads-ingest.data.gov.sg/d_ea9ed51da2787afaf8e51f827c304208/256c84a6d38a8666e7bd72fbdd36fae6c641aa088852dcd4004941f3d648094e.csv?AWSAccessKeyId=ASIAU7LWPY2WJXVJUUUU&Expires=1771227038&Signature=VTcS2HdMLRE2CFfUHCyqv%2B70XxM%3D&X-Amzn-Trace-Id=Root%3D1-6992b98d-6c6806174de93c242101f6ef%3BParent%3D2b164c9c6a81a5dd%3BSampled%3D0%3BLineage%3D1%3Affb76583%3A0&response-content-disposition=attachment%3B%20filename%3D%22ResaleFlatPricesBasedonRegistrationDateFromJan2015toDec2016.csv%22&x-amz-security-token=IQoJb3JpZ2luX2VjEGcaDmFwLXNvdXRoZWFzdC0xIkcwRQIhANd8KLS2uwNNgDUS9ogKi65J6kgDgHx9%2FOJQkxvzfjD%2FAiAphqh60swxdXCF%2BkIcSvHHbeZp4%2BI1DXo0nqe7cr%2BuKiqqAwgwEAQaDDM0MjIzNTI2ODc4MCIM3IVQGBkm68%2FGNcxXKocDp2Zh3O4EsEvEx%2FhPH2tHV244JDIpAO%2Fo1eNuogSgz6nHSC7uAsFD3z%2B5sVaz9s4M8xfQg35ZzDsCqOXz6sfJBmdkDPNY4lpX%2FQMZpGEqsZtEINqdF5uIvPkUPg4RtVdr0eWbsyS%2FdYhl76E%2BBgBdmr3LMwXV%2B0f7OKUMpRrt6JjYoITbLZKtV1H1v1r9xqocXQfc%2Flmhv5KDu88e%2F7FQX0J8nFZqKIoxRNgWX6Jc91htAwuLUNHmhGhuPee01JYMQdoA3sXCr0XBy1tLjLtrU%2BdvCmaYew3zRVdli6yLWKQzy2a2DmCXJQtZftrUFPC1JfZ8m8m8gGT2Wkdu%2FrYEIW9oxzgGvZQ9fRMoUc00Bk%2FVaxKj6feC3li1UEll7r%2B%2FTdS%2FEmc%2FUT%2BNpjPNKgD0bLH%2FJMzc%2FLCz%2FCsuzViH6W5dib%2FY3jE3HZDOzNrbeO2Mk86LvCBtaLzMZUESgzaOBj%2BoApbGPaNnLyUEo0prC7GBplumqaP3Fngv%2FbsbuuZVMVCrye8nzzDk8srMBjqdAQ6iPdafayxvuCTLD%2FhWKkWR6aVacYerV8T2%2F38IS3GOeZ2CF2fJB4Ydx6K9qvLf9%2FIxr9%2Bp%2BA7LuolEv8ia1ylhyG2ar8CCy5%2FJG9lo3%2Fs9kxbd3OdQRJK43szmG%2F1v6jxXtNzwZlLydroBB%2Bajd7OWpdnHMNxt9Kx45QJIwi0Ve8%2FpqTL50ZPFu0u%2FmVCzUajiUVA1tblZ0H3js7U%3D\n",
      "âœ… Combined DataFrame shape: (89356, 11)\n",
      "ðŸ“ Raw combined data saved at: HDB_Data\\Raw\\hdb_raw.csv\n"
     ]
    }
   ],
   "source": [
    "# =========================================\n",
    "# STEP 2: DOWNLOAD AND COMBINE DATA\n",
    "# =========================================\n",
    "\n",
    "import time\n",
    "from io import StringIO\n",
    "\n",
    "# ---------------------------\n",
    "# CONFIG\n",
    "# ---------------------------\n",
    "dataset_ids = [\n",
    "    \"d_2d5ff9ea31397b66239f245f57751537\", # 2012-2014\n",
    "    \"d_ea9ed51da2787afaf8e51f827c304208\"  # 2015-2016\n",
    "]\n",
    "\n",
    "api_key = \"v2:6f97f44df9fc11c3c0a00439686ce9a8f8be7e40920eec3766f3d869cf3af4d0:v9K8fVHSuMhujPerUww5tZhTQUuZGEpc\"\n",
    "\n",
    "# Optional: leave empty for full dataset\n",
    "payload = {\"columnNames\": [], \"filters\": []}\n",
    "\n",
    "headers = {\n",
    "    \"Authorization\": f\"Bearer {api_key}\",\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}\n",
    "\n",
    "base_url = \"https://api-open.data.gov.sg/v1/public/api/datasets\"\n",
    "\n",
    "# ---------------------------\n",
    "# FUNCTIONS\n",
    "# ---------------------------\n",
    "\n",
    "def initiate_download(dataset_id, max_retries=10, wait_sec=10):\n",
    "    url = f\"{base_url}/{dataset_id}/initiate-download\"\n",
    "    for attempt in range(max_retries):\n",
    "        response = requests.get(url, headers=headers, json=payload)\n",
    "        if response.status_code in [200, 201]:\n",
    "            print(f\"[{dataset_id}] Download initiated.\")\n",
    "            return True\n",
    "        elif response.status_code == 429:\n",
    "            print(f\"[{dataset_id}] Initiate rate limit hit. Waiting {wait_sec}s... (Attempt {attempt+1}/{max_retries})\")\n",
    "            time.sleep(wait_sec)\n",
    "        else:\n",
    "            raise Exception(f\"Initiate failed for {dataset_id}: {response.text}\")\n",
    "    raise Exception(f\"[{dataset_id}] Initiate failed after {max_retries} retries due to rate limit.\")\n",
    "\n",
    "def poll_download(dataset_id, max_retries=30, wait_sec=5):\n",
    "    url = f\"{base_url}/{dataset_id}/poll-download\"\n",
    "    for i in range(max_retries):\n",
    "        response = requests.get(url, headers=headers, json=payload)\n",
    "        if response.status_code in [200, 201]:\n",
    "            data = response.json().get(\"data\", {})\n",
    "            file_url = data.get(\"url\")\n",
    "            if file_url:\n",
    "                print(f\"[{dataset_id}] File ready: {file_url}\")\n",
    "                return file_url\n",
    "            print(f\"[{dataset_id}] File not ready yet. Retry {i+1}/{max_retries}...\")\n",
    "        elif response.status_code == 429:\n",
    "            print(f\"[{dataset_id}] Poll rate limit hit. Waiting {wait_sec}s...\")\n",
    "        else:\n",
    "            raise Exception(f\"Poll failed for {dataset_id}: {response.text}\")\n",
    "        time.sleep(wait_sec)\n",
    "    raise Exception(f\"[{dataset_id}] File not ready after {max_retries} retries.\")\n",
    "\n",
    "def download_to_dataframe(file_url):\n",
    "    r = requests.get(file_url)\n",
    "    r.raise_for_status()\n",
    "    return pd.read_csv(StringIO(r.text))\n",
    "\n",
    "# ---------------------------\n",
    "# MAIN\n",
    "# ---------------------------\n",
    "\n",
    "dataframes = []\n",
    "\n",
    "for dataset_id in dataset_ids:\n",
    "    try:\n",
    "        initiate_download(dataset_id)\n",
    "        file_url = poll_download(dataset_id)\n",
    "        df = download_to_dataframe(file_url)\n",
    "        dataframes.append(df)\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "\n",
    "if dataframes:\n",
    "    combined_df = pd.concat(dataframes, ignore_index=True)\n",
    "    print(f\"âœ… Combined DataFrame shape: {combined_df.shape}\")\n",
    "\n",
    "    # ---------------------------\n",
    "    # SAVE RAW DATA\n",
    "    # ---------------------------\n",
    "    combined_df.to_csv(raw_file, index=False)  # use raw_file variable\n",
    "    print(f\"ðŸ“ Raw combined data saved at: {raw_file}\")\n",
    "\n",
    "else:\n",
    "    combined_df = pd.DataFrame()\n",
    "    print(\"âš ï¸ No data downloaded.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7969a598-0440-48d7-b74e-a0efb3f5015e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== ETL SANITY CHECK =====\n",
      "\n",
      "1ï¸âƒ£ Total Rows Loaded: 74289\n",
      "\n",
      "2ï¸âƒ£ Columns Present:\n",
      "   1. month\n",
      "   2. town\n",
      "   3. flat_type\n",
      "   4. block\n",
      "   5. street_name\n",
      "   6. storey_range\n",
      "   7. floor_area_sqm\n",
      "   8. flat_model\n",
      "   9. lease_commence_date\n",
      "   10. resale_price\n",
      "   11. remaining_lease\n",
      "   12. remaining_lease_years\n",
      "   13. remaining_lease_months\n",
      "   14. avg_resale_price\n",
      "   15. resale_identifier\n",
      "   16. resale_identifier_hashed\n",
      "\n",
      "\n",
      "3ï¸âƒ£ Column Data Types:\n",
      "month                       datetime64[ns]\n",
      "town                                object\n",
      "flat_type                           object\n",
      "block                               object\n",
      "street_name                         object\n",
      "storey_range                        object\n",
      "floor_area_sqm                     float64\n",
      "flat_model                          object\n",
      "lease_commence_date                  int64\n",
      "resale_price                       float64\n",
      "remaining_lease                    float64\n",
      "remaining_lease_years                int64\n",
      "remaining_lease_months               int64\n",
      "avg_resale_price                   float64\n",
      "resale_identifier                   object\n",
      "resale_identifier_hashed            object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# =========================================\n",
    "# STEP 2.5: ETL SANITY PROFILING\n",
    "# Purpose: Verify combined dataset structure before cleaning\n",
    "# =========================================\n",
    "\n",
    "print(\"===== ETL SANITY CHECK =====\\n\")\n",
    "\n",
    "# 1ï¸âƒ£ Total Rows\n",
    "total_rows = len(combined_df)\n",
    "print(f\"1ï¸âƒ£ Total Rows Loaded: {total_rows}\\n\")\n",
    "\n",
    "# 2ï¸âƒ£ Column Names\n",
    "columns = combined_df.columns.tolist()\n",
    "print(\"2ï¸âƒ£ Columns Present:\")\n",
    "for idx, col in enumerate(columns, start=1):\n",
    "    print(f\"   {idx}. {col}\")\n",
    "print(\"\\n\")\n",
    "\n",
    "# 3ï¸âƒ£ Data Types\n",
    "print(\"3ï¸âƒ£ Column Data Types:\")\n",
    "print(combined_df.dtypes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2ce52012-8aff-483c-8333-2fd0556a02db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\KoYa816\\AppData\\Local\\Temp\\ipykernel_5760\\1916378369.py:71: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  anomalies = combined_df.groupby(['town', 'flat_type']).apply(flag_outliers).reset_index(level=[0,1], drop=True)\n",
      "C:\\Users\\KoYa816\\AppData\\Local\\Temp\\ipykernel_5760\\1916378369.py:72: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  failed_df = pd.concat([failed_df, combined_df[anomalies]], ignore_index=True)\n",
      "C:\\Users\\KoYa816\\AppData\\Local\\Temp\\ipykernel_5760\\1916378369.py:73: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  combined_df = combined_df[~anomalies]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Data Cleaning Complete\n",
      "ðŸ“ Cleaned data saved at: HDB_Data\\Cleaned\\hdb_cleaned.csv\n",
      "ðŸ“ Failed data appended to: HDB_Data\\Failed\\hdb_failed.csv\n",
      "Total Valid Rows: 74152\n",
      "Total Failed Rows: 17210\n"
     ]
    }
   ],
   "source": [
    "# =========================================\n",
    "# STEP 3: DATA VALIDATION & CLEANING\n",
    "# Purpose: Remove invalid rows, compute remaining lease, deduplicate, and track failed records\n",
    "# =========================================\n",
    "\n",
    "# -------------------------------\n",
    "# Load or Initialize Failed Dataset\n",
    "# -------------------------------\n",
    "# Use the already defined failed_file path from Step 1\n",
    "if os.path.exists(failed_file):\n",
    "    failed_df = pd.read_csv(failed_file)\n",
    "else:\n",
    "    failed_df = pd.DataFrame(columns=combined_df.columns)\n",
    "\n",
    "# -------------------------------\n",
    "# 1ï¸âƒ£ Month Validation\n",
    "# -------------------------------\n",
    "def validate_month(x):\n",
    "    try:\n",
    "        pd.to_datetime(x)\n",
    "        return True\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "month_valid = combined_df['month'].apply(validate_month)\n",
    "failed_df = pd.concat([failed_df, combined_df[~month_valid]], ignore_index=True)\n",
    "combined_df = combined_df[month_valid]\n",
    "\n",
    "# -------------------------------\n",
    "# 2ï¸âƒ£ Non-null String Validation\n",
    "# -------------------------------\n",
    "categorical_cols = ['town', 'flat_type', 'flat_model', 'storey_range']\n",
    "for col in categorical_cols:\n",
    "    valid = combined_df[col].notnull() & combined_df[col].astype(str).str.strip().ne('')\n",
    "    failed_df = pd.concat([failed_df, combined_df[~valid]], ignore_index=True)\n",
    "    combined_df = combined_df[valid]\n",
    "\n",
    "# -------------------------------\n",
    "# 3ï¸âƒ£ Remaining Lease Calculation\n",
    "# -------------------------------\n",
    "lease_years = 99\n",
    "combined_df['month'] = pd.to_datetime(combined_df['month'])\n",
    "\n",
    "# Compute remaining lease in months, then convert to years & months\n",
    "combined_df['remaining_lease_total_months'] = (\n",
    "    (lease_years - (combined_df['month'].dt.year - combined_df['lease_commence_date'])) * 12\n",
    ")\n",
    "combined_df['remaining_lease_years'] = combined_df['remaining_lease_total_months'] // 12\n",
    "combined_df['remaining_lease_months'] = combined_df['remaining_lease_total_months'] % 12\n",
    "combined_df.drop(columns=['remaining_lease_total_months'], inplace=True)\n",
    "\n",
    "# -------------------------------\n",
    "# 4ï¸âƒ£ Deduplicate by Composite Key (all columns except resale_price)\n",
    "# -------------------------------\n",
    "key_cols = [col for col in combined_df.columns if col != 'resale_price']\n",
    "combined_df.sort_values('resale_price', ascending=False, inplace=True)\n",
    "duplicates = combined_df.duplicated(subset=key_cols, keep='first')\n",
    "failed_df = pd.concat([failed_df, combined_df[duplicates]], ignore_index=True)\n",
    "combined_df = combined_df[~duplicates]\n",
    "\n",
    "# -------------------------------\n",
    "# 5ï¸âƒ£ Anomalous Resale Price Detection (3-sigma heuristic)\n",
    "# -------------------------------\n",
    "def flag_outliers(group):\n",
    "    mean = group['resale_price'].mean()\n",
    "    std = group['resale_price'].std()\n",
    "    upper = mean + 3*std\n",
    "    lower = mean - 3*std\n",
    "    return ~group['resale_price'].between(lower, upper)\n",
    "\n",
    "anomalies = combined_df.groupby(['town', 'flat_type']).apply(flag_outliers).reset_index(level=[0,1], drop=True)\n",
    "failed_df = pd.concat([failed_df, combined_df[anomalies]], ignore_index=True)\n",
    "combined_df = combined_df[~anomalies]\n",
    "\n",
    "# Additional check: floor_area_sqm > 0\n",
    "invalid_floor = combined_df['floor_area_sqm'] <= 0\n",
    "failed_df = pd.concat([failed_df, combined_df[invalid_floor]], ignore_index=True)\n",
    "combined_df = combined_df[~invalid_floor]\n",
    "\n",
    "# -------------------------------\n",
    "# 6ï¸âƒ£ Save Cleaned and Failed Datasets\n",
    "# -------------------------------\n",
    "combined_df.to_csv(cleaned_file, index=False)\n",
    "failed_df.to_csv(failed_file, index=False)\n",
    "\n",
    "print(\"âœ… Data Cleaning Complete\")\n",
    "print(f\"ðŸ“ Cleaned data saved at: {cleaned_file}\")\n",
    "print(f\"ðŸ“ Failed data appended to: {failed_file}\")\n",
    "print(\"Total Valid Rows:\", len(combined_df))\n",
    "print(\"Total Failed Rows:\", len(failed_df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0aa74882-a53b-4b22-8427-9cd919ddde47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Transformation Complete\n",
      "ðŸ“ Transformed data saved at: HDB_Data\\Transformed\\hdb_transformed.csv\n",
      "ðŸ“ Hashed data saved at: HDB_Data\\Hashed\\hdb_hashed.csv\n",
      "ðŸ“ Failed data updated at: HDB_Data\\Failed\\hdb_failed.csv\n",
      "Total Valid Rows: 72777\n",
      "Total Failed Rows: 18585\n"
     ]
    }
   ],
   "source": [
    "# =========================================\n",
    "# STEP 4: DATA TRANSFORMATION\n",
    "# Purpose: Create Resale Identifier and Hashed Identifier\n",
    "# =========================================\n",
    "\n",
    "# -------------------------------\n",
    "# Load existing failed dataset if not already loaded\n",
    "# -------------------------------\n",
    "if 'failed_df' not in globals():\n",
    "    if os.path.exists(failed_file):\n",
    "        failed_df = pd.read_csv(failed_file)\n",
    "    else:\n",
    "        failed_df = pd.DataFrame(columns=combined_df.columns)\n",
    "\n",
    "# -------------------------------\n",
    "# 1ï¸âƒ£ Compute Average Resale Price per Month-Town-Flat Type\n",
    "# -------------------------------\n",
    "avg_price = combined_df.groupby(['month', 'town', 'flat_type'])['resale_price'].mean().reset_index()\n",
    "avg_price.rename(columns={'resale_price': 'avg_resale_price'}, inplace=True)\n",
    "combined_df = combined_df.merge(avg_price, on=['month', 'town', 'flat_type'], how='left')\n",
    "\n",
    "# -------------------------------\n",
    "# 2ï¸âƒ£ Helper Functions for Resale Identifier\n",
    "# -------------------------------\n",
    "def block_digits(block):\n",
    "    digits = ''.join(filter(str.isdigit, str(block)))  # keep digits only\n",
    "    return digits.zfill(3)[:3]\n",
    "\n",
    "def avg_price_digits(price):\n",
    "    price_str = str(int(round(price)))\n",
    "    return price_str[:2].zfill(2)\n",
    "\n",
    "def month_digits(date):\n",
    "    return f\"{date.month:02d}\"\n",
    "\n",
    "def town_char(town):\n",
    "    return str(town)[0].upper()\n",
    "\n",
    "# -------------------------------\n",
    "# 3ï¸âƒ£ Create Resale Identifier\n",
    "# -------------------------------\n",
    "combined_df['resale_identifier'] = (\n",
    "    \"S\" +\n",
    "    combined_df['block'].apply(block_digits) +\n",
    "    combined_df['avg_resale_price'].apply(avg_price_digits) +\n",
    "    combined_df['month'].apply(month_digits) +\n",
    "    combined_df['town'].apply(town_char)\n",
    ")\n",
    "\n",
    "# -------------------------------\n",
    "# 4ï¸âƒ£ Deduplicate by Resale Identifier (keep highest resale_price)\n",
    "# -------------------------------\n",
    "combined_df.sort_values('resale_price', ascending=False, inplace=True)\n",
    "duplicates = combined_df.duplicated(subset=['resale_identifier'], keep='first')\n",
    "\n",
    "# Append duplicates to failed dataset\n",
    "failed_df = pd.concat([failed_df, combined_df[duplicates]], ignore_index=True)\n",
    "\n",
    "# Keep only unique Resale Identifiers\n",
    "combined_df = combined_df[~duplicates]\n",
    "\n",
    "# -------------------------------\n",
    "# 5ï¸âƒ£ Hash Resale Identifier (SHA-256)\n",
    "# -------------------------------\n",
    "combined_df['resale_identifier_hashed'] = combined_df['resale_identifier'].apply(\n",
    "    lambda x: hashlib.sha256(x.encode()).hexdigest()\n",
    ")\n",
    "\n",
    "# -------------------------------\n",
    "# 6ï¸âƒ£ Save Outputs\n",
    "# -------------------------------\n",
    "combined_df.to_csv(transformed_file, index=False)  # Transformed\n",
    "combined_df.copy().to_csv(hashed_file, index=False)  # Hashed\n",
    "failed_df.to_csv(failed_file, index=False)  # Updated failed\n",
    "\n",
    "# -------------------------------\n",
    "# 7ï¸âƒ£ Summary\n",
    "# -------------------------------\n",
    "print(\"âœ… Transformation Complete\")\n",
    "print(f\"ðŸ“ Transformed data saved at: {transformed_file}\")\n",
    "print(f\"ðŸ“ Hashed data saved at: {hashed_file}\")\n",
    "print(f\"ðŸ“ Failed data updated at: {failed_file}\")\n",
    "print(\"Total Valid Rows:\", len(combined_df))\n",
    "print(\"Total Failed Rows:\", len(failed_df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9175c798-c041-491b-8158-31be009c1800",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
